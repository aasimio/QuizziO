# Task 6.4: Performance Profiling & Optimization — Execution Plan (Codex)

> Scope note: This document is a **plan only** (no implementation yet). It is written to execute Task **6.4** from `Tasks/Development-Plan-v2_3.md` and meet PRD NFR-P targets in `Tasks/PRD.md` §6.1.

## Overview
Task 6.4 ensures QuizziO’s camera → OMR pipeline is fast, smooth, and memory-safe on real devices (especially low-end Android API 24), using **profiling-first** methodology:

1. Measure baseline performance (scan pipeline, marker detection, cold start, memory).
2. Identify bottlenecks with tooling + in-app instrumentation.
3. Apply the smallest, highest-impact optimizations.
4. Re-measure, then validate on low-end hardware.

This plan intentionally prioritizes **repeatable measurements** and **clear start/stop definitions** so improvements are measurable and regression-resistant.

## Requirements (locked targets)
From `Tasks/PRD.md` §6.1 Performance and `Tasks/Development-Plan-v2_3.md` Task 6.4:

| ID | Metric | Target | Interpretation |
|---|---|---:|---|
| NFR-P-01 | Full scan pipeline completion | < 500ms | From capture to result display |
| NFR-P-02 | Preview marker detection | < 100ms/frame | Supports ~10 FPS detection |
| NFR-P-03 | App cold start time | < 3s | App launch → camera ready |
| NFR-P-04 | Memory usage during scan | < 200MB | Peak allocation during scan session |
| NFR-P-05 | Battery consumption | < 5%/hour scanning | Not listed in 6.4 checklist, but present in PRD |

**Task 6.4 “Done when”**: All NFR-P targets met (PRD 6.1). (Battery can be treated as “stretch” unless explicitly required for 6.4 sign-off; it is required for 7.7 final sign-off in the dev plan.)

## Current Code Audit (where time/memory is spent)
Key runtime paths (v2.3):

### Preview marker detection path (per-frame)
- `lib/features/omr/presentation/bloc/scanner_bloc.dart`
  - `_processFrame(CameraImage image)`:
    1. `CameraService.convertCameraImageToBytes(image)` (copies/crops padding in some cases)
    2. `ImagePreprocessor.createMatFromPixels(...)` (alloc + copy into `cv.Mat`)
    3. `ImagePreprocessor.preprocess(mat)` (cvtColor + CLAHE + normalize)
    4. `MarkerDetector.detect(grayscale)` (ArUco detectMarkers + Dart-side mapping)
  - Throttle is `_frameInterval = 100ms` (~10 FPS) and `_isProcessingFrame` gate.

### Full scan pipeline path (high-res still)
- `lib/features/omr/presentation/bloc/scanner_bloc.dart`
  - `_onStabilityAchieved`:
    - stops stream, emits `ScannerCapturing`, calls `CameraService.captureImage()` (camera still capture)
  - `_onImageCaptured`:
    - `TemplateManager.getTemplate(...)` (cached)
    - `OmrPipeline.process(imageBytes, ...)` (decode → preprocess → detect → warp → read bubbles → threshold → extract)
    - `GradingService.grade(...)` (pure Dart)
    - `_extractNameRegion(imageBytes, template)` **re-runs** decode → preprocess → detect → warp, then crops + `cv.imencode('.png', ...)`
    - `ScanRepository.save(scanResult)` (Hive write)
    - emits `ScannerResult` (UI shows popup)

### Cold start path (to camera ready)
- `lib/main.dart`: DI + Hive init, then `runApp`.
- `lib/app.dart`: starts at `QuizzesPage` (not scan page).
- First entry into scanning:
  - `ScanPapersPage` creates `ScannerBloc` and triggers `ScannerInitCamera`
  - `CameraService.initialize()` → camera init and `lockCaptureOrientation`
  - `MarkerDetector.initialize()`
  - `CameraService.startImageStream()`; UI becomes usable once preview renders + state is `ScannerPreviewing`.

### Likely hotspots / risks (to confirm by profiling)
- Preview preprocess uses CLAHE+normalize for every frame (expensive; might not be needed for ArUco).
- Multiple copies/allocations per frame (`Uint8List.fromList`, `setRange`, temporary Mats).
- Full scan duplicates marker detection + perspective warp for name region extraction.
- `CameraService.initialize()` does not guard against re-init / dispose old controller (potential lifecycle/memory risk if scan page is reopened).
- Native memory (OpenCV Mats) won’t always show in Dart heap charts; OS-level profiling is required for NFR-P-04.

## Profiling Principles (to avoid misleading results)
1. **Profile in `--profile` or `--release`**. Debug mode is not valid for timing targets.
2. **Always run multiple iterations** and report at least: average + p50 + p90/p95.
3. **Control variables**: same device, same sheet, similar lighting, no background downloads, device not thermally throttled.
4. **Measure where the user feels time** (end-to-end) and where the CPU spends time (step-level).
5. **Prefer timeline events** (`dart:developer`) over `print` spam; logs distort timing.

## Tooling Setup
### Flutter tooling
- Run builds:
  - `flutter run --profile`
  - `flutter run --release` (for final validation passes; harder to attach profilers)
- Flutter DevTools:
  - Performance (UI thread, raster thread, frame times)
  - CPU Profiler (Dart)
  - Memory (Dart heap + GC behavior)

### Platform profilers (needed for NFR-P-04)
- **Android Studio Profiler**:
  - CPU profiler (native + threads)
  - Memory profiler (native allocations)
- **ADB / dumpsys** (quick, scriptable checks):
  - `adb shell dumpsys meminfo <package>`
  - `adb shell am force-stop <package>` (cold start repeatability)
  - `adb shell am start -W <intent>` (startup timing baselines)

### iOS (optional but recommended)
- Instruments: Time Profiler + Allocations.

## Instrumentation Plan (what to add before measuring)
Goal: get consistent timestamps and breakdowns without permanently affecting release performance.

### 1) Add a small “perf” utility (gated)
Create a lightweight helper (conceptually) that:
- Is enabled only in `kProfileMode` / `kDebugMode` (or via a compile-time flag).
- Records step timings in-memory (ring buffer) and prints summary on demand (e.g., every N scans).
- Emits `dart:developer` timeline events for DevTools tracing.

Suggested instrumentation mechanisms:
- `Stopwatch` for simple durations.
- `dart:developer.Timeline` / `TimelineTask` for step spans visible in DevTools.

### 2) Define canonical measurement boundaries (avoid ambiguity)
Because “capture → result display” is ambiguous in Flutter camera flows, record **three** timings:

1. **Capture latency**: `ScannerStabilityAchieved` handler start → `CameraService.captureImage()` returns bytes.
2. **Processing latency**: start of `_onImageCaptured` → emit `ScannerResult` (or dispatch `ScannerProcessingComplete`).
3. **End-to-end**: emit `ScannerCapturing` → first frame of `ScannerResult` UI (or state emission timestamp if UI timestamp is hard).

For NFR-P-01, agree on which metric is the official one:
- If PRD literally means “user-perceived after shutter”, use **End-to-end**.
- If PRD intends “processing time after capture bytes available”, use **Processing latency**.
Record all three either way; pick the official one once validated with stakeholders.

### 3) Add step-level spans in the two hot paths
#### Preview path (`ScannerBloc._processFrame`)
Measure:
- `frame_convert_bytes_ms`
- `frame_mat_create_ms`
- `frame_preprocess_ms` (break out `cvtColor`, `CLAHE`, `normalize` if possible)
- `frame_marker_detect_ms`
- `frame_total_ms`

Also record metadata for correlation:
- format: `yuv420` vs `bgra8888`
- width/height
- frame dropped reasons (throttle vs busy)

#### Full scan path (`OmrPipeline.process` + `_extractNameRegion`)
Measure:
- `decode_ms` (`cv.imdecode`)
- `preprocess_ms`
- `marker_detect_ms`
- `corner_resolve_ms` (note: currently re-detects)
- `warp_ms`
- `bubble_read_ms` (expected to scale with template size)
- `threshold_ms`
- `extract_answers_ms`
- `grade_ms`
- `name_region_total_ms` (and inside it: decode/preprocess/detect/warp/crop/encode)
- `persist_ms` (Hive write)
- `total_processing_ms` (already present as `processingTimeMs`, but needs step breakdown)

### 4) Cold start instrumentation
Record:
- `t_main_start` at the very top of `main()`
- `t_runApp_called`
- `t_first_frame_quizzes` (use `WidgetsBinding.instance.addPostFrameCallback`)
- `t_scan_page_opened` (on navigation to `ScanPapersPage`)
- `t_camera_initialize_done` (after `CameraService.initialize`)
- `t_preview_visible` (when `CameraPreview` is rendering + bloc state is previewing)

Define “camera ready” for NFR-P-03 as:
- `t_camera_initialize_done` OR `t_preview_visible` (prefer the latter since it’s user-perceived).

### 5) Memory instrumentation (lightweight)
In addition to external profilers, record coarse RSS samples (best-effort):
- Capture `ProcessInfo.currentRss` (Dart) at key points:
  - before capture
  - after pipeline finished
  - after returning to previewing
This won’t perfectly map OpenCV native memory, but it’s useful for trend detection (leaks/regressions).

## Execution Steps (mapped to Task 6.4 sub-tasks)

### 6.4.1 Profile scan pipeline (capture → result)
**Goal:** Validate NFR-P-01 (<500ms) and locate the top 1–3 bottlenecks.

Procedure:
1. Build and run on a mid-range baseline device in `--profile`.
2. Enable perf instrumentation and collect at least:
   - 30 scans on `std_10q`
   - 30 scans on `std_20q`
   - 30 scans on `std_50q` (worst case)
3. For each scan, store the three timings (capture / processing / end-to-end) plus step breakdown.
4. Summarize results as: average, p50, p95 for each template and each step.
5. If NFR-P-01 is missed, identify whether the regression is:
   - camera still capture latency (`takePicture`)
   - OpenCV decode/preprocess/warp
   - bubble reading (ROI loop)
   - duplicated name region extraction path
   - persistence

Outputs:
- A short performance report (see “Deliverables”) with tables per template.
- A prioritized bottleneck list with estimated impact.

### 6.4.2 Profile marker detection (per-frame)
**Goal:** Validate NFR-P-02 (<100ms/frame) while maintaining stable UX (no flicker, stable auto-capture).

Procedure:
1. Run `ScanPapersPage` in `--profile` with instrumentation enabled.
2. Hold a real sheet in view (stable markers) and record timings over ~300 frames (or ~30 seconds at ~10 FPS).
3. Repeat with:
   - markers partially occluded
   - dim lighting
   - bright lighting
4. Summarize `frame_total_ms` p50/p95 and each sub-step.

Decision gates:
- If preprocess is >50% of frame time, evaluate reducing preprocessing work for preview frames.
- If marker detect is dominant, investigate downscaling or corner-ROI detection.

### 6.4.3 Profile cold start (launch → camera ready)
**Goal:** Validate NFR-P-03 (<3s).

Clarify the scenario first (pick one and report both if feasible):
- Scenario A (strict): app cold start → user reaches scan page → camera preview visible.
- Scenario B (practical): app cold start directly into scan (if you later add a shortcut).

Procedure (Scenario A):
1. Force stop app.
2. Launch app; navigate to scan flow using a fixed path (same taps).
3. Record timestamps: `t_main_start` → `t_preview_visible`.
4. Repeat 10 times; compute average + p95.

Notes:
- Because navigation adds variance, also record `t_scan_page_opened` → `t_preview_visible` (camera init only).

### 6.4.4 Profile memory (peak allocation during scan)
**Goal:** Validate NFR-P-04 (<200MB peak).

Procedure:
1. Use Android Studio Memory profiler on Android:
   - run continuous scanning sessions (e.g., 50 scans back-to-back)
   - watch for monotonic memory growth (native leak symptom)
2. Use `adb shell dumpsys meminfo <package>` snapshots:
   - idle on quizzes page
   - scan preview running (stream active)
   - immediately after a scan completes
   - after returning to preview
3. Repeat on at least `std_50q` template (worst case).
4. If memory exceeds 200MB or grows over time:
   - confirm every `cv.Mat` is disposed on all paths
   - verify camera controller lifecycle (re-init/dispose)
   - check for retained `Uint8List` / images in UI state

### 6.4.5 Optimize if targets missed (playbook)
Apply optimizations in this order (highest expected ROI, lowest risk), re-measuring after each change:

#### A) Remove duplicated OpenCV work in full scan flow (likely biggest win)
Current duplication:
- `OmrPipeline.process(...)` does decode+preprocess+detect+warp.
- `_extractNameRegion(...)` repeats decode+preprocess+detect+warp again.

Optimization options:
1. Extend `OmrPipeline.process` to also return the name-region bytes (or aligned Mat temporarily) so name cropping happens from the already-warped image.
2. If keeping separate, refactor marker detection to expose corners from the first detection to avoid re-detecting.

Expected impact: large reduction in end-to-end latency + native allocations.

#### B) Make preview marker detection cheaper
Options:
1. Skip CLAHE/normalize for preview frames (ArUco generally works on plain grayscale).
2. Downscale the frame before detection (e.g., 1/2 or 1/3), then scale coordinates back if needed.
3. Detect markers only in corner ROIs where markers are expected (crop 4 rectangles, detect inside them).
4. Reuse expensive objects (e.g., CLAHE instance) instead of creating them per frame.

Expected impact: improves p95 frame times and stability detection.

#### C) Reduce allocations / copies in the preview pipeline
Options:
1. Avoid `Uint8List.fromList(...)` when the camera plane bytes are already contiguous.
2. Reuse buffers or introduce a simple pool for per-frame byte arrays if needed.
3. Avoid unnecessary Mat clones in preprocess when already grayscale.

Expected impact: less GC pressure and smoother preview.

#### D) Bubble reading optimization (only if needed)
If `bubble_read_ms` dominates on `std_50q`:
1. Investigate computing means in fewer native calls (batch ROI processing).
2. Evaluate reducing template resolution (smaller warp target) while validating accuracy.

Expected impact: improves worst-case template processing.

#### E) Camera capture bottleneck mitigation (if `takePicture` dominates)
If capture latency alone breaks the 500ms target:
1. Consider using the latest high-quality preview frame as the “capture” input (skip still capture), if accuracy remains acceptable.
2. Consider adjusting camera preset or capture format.
3. Validate the PRD interpretation of “capture” (processing vs shutter-to-result).

Expected impact: essential if camera still capture is the limiting factor.

#### F) Isolates / threading (mostly for UX, sometimes for throughput)
Move CPU-heavy work off the UI isolate to prevent jank (even if total time is similar):
- Run scan pipeline in a background isolate (verify `opencv_dart` behavior in isolates).
- Keep UI responsive with progress overlay updates.

Expected impact: smoother UX; may help if UI contention affects measured “result display” time.

### 6.4.6 Validate on low-end device (Android API 24, 8MP camera)
**Goal:** Confirm targets hold on minimum supported hardware.

Device profile (minimum):
- Android 7.0 (API 24)
- 2–3GB RAM
- 8MP camera

Procedure:
1. Install a `--profile` build first to measure; then re-check in `--release`.
2. Repeat 6.4.1–6.4.4 with smaller sample sizes if time-constrained (e.g., 10 scans per template), but always include `std_50q`.
3. Record thermal state notes (device warm/cool) and rerun if throttling is suspected.

## Deliverables (what to produce when executing this plan)
1. `Tasks/6.4_performance_report.md` (new):
   - device matrix + OS versions
   - tables for NFR-P-01..P-04 (and P-05 if measured)
   - p50/p95 for each template
   - identified bottlenecks + applied optimizations
2. In-app instrumentation (gated) + timeline spans sufficient to repeat measurements later.
3. Update `Tasks/Development-Plan-v2_3.md` checkboxes for 6.4.1–6.4.6 as each is completed (follow `Tasks/process-task-list.md` protocol when actually executing).

## Definition of Done (Task 6.4)
- NFR-P-01..P-04 meet targets on at least one mid-range device and on the low-end Android API 24 device.
- Measurements are reproducible (documented procedure + reported p50/p95).
- No evidence of memory leaks during repeated scanning sessions (stable native memory/RSS trends).
- Clear notes exist for any deferred item (e.g., battery measurement moved to Task 7.7 if agreed).

